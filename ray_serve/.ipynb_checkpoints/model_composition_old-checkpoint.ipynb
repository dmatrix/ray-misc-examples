{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Model Composition ServerHandle APIs\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### Learning Objective:\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    " * compose complex models using ServeHandle APIs\n",
    " * deploy each discreate model as a seperate model deployment\n",
    " * use a single class deployment to include individual as a single model composition\n",
    " * deploy and serve this singluar model composition\n",
    "\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer ü§ó to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern using ServeHandle APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example üê¶ tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our function deployment model to analyse the tweet using a pretrained transformer from HuggingFace ü§ó.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "def sentiment_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, task=\"sentiment-analysis\")\n",
    "\n",
    "    return pipeline(text)[0]['label'], pipeline(text)[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our function to translate a tweet from English --> French using a pretrained Transformer from HuggingFace ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a tweet from English --> French \n",
    "# using a pretrained Transformer from HuggingFace\n",
    "@serve.deployment(num_replicas=2)\n",
    "def translate_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "    model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "    use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "    pipeline = TranslationPipeline(model, tokenizer, task=\"translation_en_to_fr\", device=use_gpu)\n",
    "\n",
    "    return pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "<img src=\"images/tweet_composition.png\" width=\"60%\" height=\"25%\">\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate_model.get_handle(sync=False)\n",
    "        self.sentiment_model = sentiment_model.get_handle(sync=False)\n",
    "\n",
    "    async def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "\n",
    "        sentiment, score = await(await self.sentiment_model.remote(data))\n",
    "        trans_text = await(await self.translate_model.remote(data))\n",
    "\n",
    "        return {'Sentiment': sentiment, 'score': score, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dc58b-d3a4-4ee2-876d-832983ff4642",
   "metadata": {},
   "source": [
    "Start a Ray Serve instance. Note that if Ray cluster does not exist, it will create one and attach the Ray Serve\n",
    "instance to it. If one exists it'll run on that Ray cluster instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc61315-27e0-4b1b-a4aa-ae6abd2faa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 17:24:53,669\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=71300)\u001b[0m INFO 2023-02-15 17:24:55,766 controller 71300 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:YEvAcW:SERVE_PROXY_ACTOR-38d7bf48edb1926c3334a13cc603ad4c21c738a2cdb3f2a1c7c2b803' on node '38d7bf48edb1926c3334a13cc603ad4c21c738a2cdb3f2a1c7c2b803' listening on '127.0.0.1:8000'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve._private.client.ServeControllerClient at 0x29eba9a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26264457-b699-48ff-b5a8-f9a8d66c95f6",
   "metadata": {},
   "source": [
    "### Deploy our models \n",
    "\n",
    "Deploy our models. As seen before in other tutorials, this is as simple and intuitive as invoking `<func_or_class_name>.deploy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39401d3-8fb0-410c-bb61-d25164fbeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO:     Started server process [71303]\n",
      "\u001b[2m\u001b[36m(ServeController pid=71300)\u001b[0m INFO 2023-02-15 17:24:56,533 controller 71300 deployment_state.py:1310 - Adding 1 replica to deployment 'sentiment_model'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=71300)\u001b[0m INFO 2023-02-15 17:24:58,548 controller 71300 deployment_state.py:1310 - Adding 2 replicas to deployment 'translate_model'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=71300)\u001b[0m INFO 2023-02-15 17:25:00,588 controller 71300 deployment_state.py:1310 - Adding 2 replicas to deployment 'ComposedModel'.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.deploy()\n",
    "translate_model.deploy()\n",
    "ComposedModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 11.5kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:00<00:00, 101kB/s]\n",
      "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 995kB/s] \n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m \n",
      "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 933k/268M [00:00<00:28, 9.31MB/s]\n",
      "Downloading:   3%|‚ñé         | 7.02M/268M [00:00<00:06, 39.6MB/s]\n",
      "Downloading:   6%|‚ñå         | 15.0M/268M [00:00<00:04, 58.0MB/s]\n",
      "Downloading:   8%|‚ñä         | 22.6M/268M [00:00<00:03, 64.8MB/s]\n",
      "Downloading:  11%|‚ñà         | 29.1M/268M [00:00<00:04, 57.0MB/s]\n",
      "Downloading:  15%|‚ñà‚ñç        | 39.7M/268M [00:00<00:03, 72.5MB/s]\n",
      "Downloading:  18%|‚ñà‚ñä        | 49.4M/268M [00:00<00:02, 80.0MB/s]\n",
      "Downloading:  22%|‚ñà‚ñà‚ñè       | 58.8M/268M [00:00<00:02, 84.3MB/s]\n",
      "Downloading:  25%|‚ñà‚ñà‚ñå       | 67.4M/268M [00:00<00:02, 84.9MB/s]\n",
      "Downloading:  28%|‚ñà‚ñà‚ñä       | 76.3M/268M [00:01<00:02, 86.0MB/s]\n",
      "Downloading:  32%|‚ñà‚ñà‚ñà‚ñè      | 86.1M/268M [00:01<00:02, 89.5MB/s]\n",
      "Downloading:  36%|‚ñà‚ñà‚ñà‚ñå      | 95.1M/268M [00:01<00:01, 88.7MB/s]\n",
      "Downloading:  39%|‚ñà‚ñà‚ñà‚ñâ      | 105M/268M [00:01<00:01, 88.1MB/s] \n",
      "Downloading:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 114M/268M [00:01<00:01, 91.0MB/s]\n",
      "Downloading:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 124M/268M [00:01<00:01, 89.9MB/s]\n",
      "Downloading:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 134M/268M [00:01<00:01, 93.5MB/s]\n",
      "Downloading:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 143M/268M [00:01<00:01, 89.7MB/s]\n",
      "Downloading:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 162M/268M [00:01<00:01, 90.9MB/s]\n",
      "Downloading:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 171M/268M [00:02<00:01, 88.1MB/s]\n",
      "Downloading:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 180M/268M [00:02<00:00, 90.2MB/s]\n",
      "Downloading:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 190M/268M [00:02<00:00, 93.1MB/s]\n",
      "Downloading:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 200M/268M [00:02<00:00, 94.0MB/s]\n",
      "Downloading:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 209M/268M [00:02<00:00, 92.3MB/s]\n",
      "Downloading:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 219M/268M [00:02<00:00, 90.4MB/s]\n",
      "Downloading:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 229M/268M [00:02<00:00, 93.0MB/s]\n",
      "Downloading:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 238M/268M [00:02<00:00, 93.9MB/s]\n",
      "Downloading:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 248M/268M [00:02<00:00, 95.2MB/s]\n",
      "Downloading:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 258M/268M [00:03<00:00, 91.8MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268M/268M [00:03<00:00, 85.7MB/s]\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:12,112 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 9542.8ms\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.21k/1.21k [00:00<00:00, 326kB/s]\n",
      "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]\n",
      "Downloading:   4%|‚ñé         | 28.7k/792k [00:00<00:03, 198kB/s]\n",
      "Downloading:  26%|‚ñà‚ñà‚ñã       | 209k/792k [00:00<00:00, 793kB/s] \n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792k/792k [00:00<00:00, 1.96MB/s]\n",
      "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]\n",
      "Downloading:   3%|‚ñé         | 41.0k/1.39M [00:00<00:04, 273kB/s]\n",
      "Downloading:  15%|‚ñà‚ñå        | 213k/1.39M [00:00<00:01, 801kB/s] \n",
      "Downloading:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 739k/1.39M [00:00<00:00, 2.41MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.39M/1.39M [00:00<00:00, 2.93MB/s]\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m   warnings.warn(\n",
      "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 1.12M/242M [00:00<00:21, 11.0MB/s]\n",
      "Downloading:   3%|‚ñé         | 6.13M/242M [00:00<00:06, 33.9MB/s]\n",
      "Downloading:   4%|‚ñç         | 10.3M/242M [00:00<00:06, 36.7MB/s]\n",
      "Downloading:   7%|‚ñã         | 16.7M/242M [00:00<00:04, 47.3MB/s]\n",
      "Downloading:  10%|‚ñâ         | 23.4M/242M [00:00<00:04, 54.4MB/s]\n",
      "Downloading:  12%|‚ñà‚ñè        | 29.5M/242M [00:00<00:03, 56.0MB/s]\n",
      "Downloading:  14%|‚ñà‚ñç        | 35.1M/242M [00:00<00:03, 55.3MB/s]\n",
      "Downloading:  17%|‚ñà‚ñã        | 41.3M/242M [00:00<00:03, 52.6MB/s]\n",
      "Downloading:  20%|‚ñà‚ñà        | 49.3M/242M [00:00<00:03, 60.2MB/s]\n",
      "Downloading:  23%|‚ñà‚ñà‚ñé       | 55.4M/242M [00:01<00:03, 56.8MB/s]\n",
      "Downloading:  27%|‚ñà‚ñà‚ñã       | 65.4M/242M [00:01<00:02, 69.2MB/s]\n",
      "Downloading:  31%|‚ñà‚ñà‚ñà       | 74.9M/242M [00:01<00:02, 76.4MB/s]\n",
      "Downloading:  35%|‚ñà‚ñà‚ñà‚ñç      | 84.4M/242M [00:01<00:01, 81.9MB/s]\n",
      "Downloading:  39%|‚ñà‚ñà‚ñà‚ñâ      | 94.6M/242M [00:01<00:01, 87.8MB/s]\n",
      "Downloading:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 105M/242M [00:01<00:01, 92.2MB/s] \n",
      "Downloading:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 114M/242M [00:01<00:01, 89.9MB/s]\n",
      "Downloading:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 123M/242M [00:01<00:01, 77.7MB/s]\n",
      "Downloading:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 131M/242M [00:02<00:01, 64.0MB/s]\n",
      "Downloading:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 138M/242M [00:02<00:01, 61.7MB/s]\n",
      "Downloading:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 147M/242M [00:02<00:01, 67.9MB/s]\n",
      "Downloading:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 155M/242M [00:02<00:01, 70.7MB/s]\n",
      "Downloading:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 163M/242M [00:02<00:01, 73.9MB/s]\n",
      "Downloading:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 172M/242M [00:02<00:00, 77.7MB/s]\n",
      "Downloading:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 182M/242M [00:02<00:00, 84.4MB/s]\n",
      "Downloading:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 191M/242M [00:02<00:00, 84.2MB/s]\n",
      "Downloading:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 199M/242M [00:02<00:00, 83.6MB/s]\n",
      "Downloading:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 208M/242M [00:02<00:00, 83.9MB/s]\n",
      "Downloading:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 218M/242M [00:03<00:00, 88.7MB/s]\n",
      "Downloading:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 227M/242M [00:03<00:00, 88.2MB/s]\n",
      "Downloading:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 236M/242M [00:03<00:00, 88.5MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 242M/242M [00:03<00:00, 73.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9651215672492981, 'Translated Text': \"Ce soir, j'ai √©t√© fou parce que ma m√®re ne me laisse pas jouer avec ce chien.\"}\n",
      "Sending tweet request... : Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:23,078 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 20517.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m INFO 2023-02-15 17:25:23,076 translate_model translate_model#htTORx replica.py:505 - HANDLE __call__ OK 10959.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:25:23,077 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 20511.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:24,403 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1317.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.99788898229599, 'Translated Text': \"Parfois. quand j'ennuie. je ne regarderai rien. et essayerai de convaincre l'homme.\"}\n",
      "Sending tweet request... : You little dog shit, you peed and pooed on my new carpet. Bad dog!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:28,042 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4961.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:25:28,039 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 3633.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:25:28,041 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4958.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:29,373 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1327.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.9984055161476135, 'Translated Text': \"Je n'ai pas eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression\"}\n",
      "Sending tweet request... : I would completely believe you. Dogs and little children - very innocent and open to seeing such things\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:34,463 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 6419.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:25:34,462 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 5086.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:25:34,463 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 6417.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:35,784 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1316.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9997748732566833, 'Translated Text': 'Je vous croyais tout √† fait: chiens et petits enfants - tr√®s innocents et ouverts √† ce genre de choses'}\n",
      "Sending tweet request... : You've got too much time on your paws. Go check on the skittle. under the, fridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:38,993 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4527.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m INFO 2023-02-15 17:25:38,990 translate_model translate_model#htTORx replica.py:505 - HANDLE __call__ OK 3202.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:25:38,992 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4525.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:40,323 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1325.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.9995866417884827, 'Translated Text': 'Vous avez trop de temps sur vos pattes.'}\n",
      "Sending tweet request... : You sneaky little devil, I can't live without you!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:43,460 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4465.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:25:43,457 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 3131.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:25:43,460 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 4463.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:44,751 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1286.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9949394464492798, 'Translated Text': 'Du petit diable, je ne peux pas vivre sans vous !!!'}\n",
      "Sending tweet request... : It's true what they say about dogs: they are you BEST BUDDY, no matter what!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:47,560 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4097.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m INFO 2023-02-15 17:25:47,558 translate_model translate_model#htTORx replica.py:505 - HANDLE __call__ OK 2802.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:25:47,559 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 4095.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:48,879 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1315.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9996572732925415, 'Translated Text': \"C'est vrai ce qu'ils disent sur les chiens : ils sont tu MEILLEUR BUDDY, peu importe quoi!\"}\n",
      "Sending tweet request... : This dog is way dope, just can't enough of her\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:52,092 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4529.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:25:52,090 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 3207.6ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:25:52,091 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4528.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:53,387 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1291.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.9972212314605713, 'Translated Text': 'Ce chien est assez dope, il ne peut pas assez de lui'}\n",
      "Sending tweet request... : This dog is way cool, just can't enough of her\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:25:56,269 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4174.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:25:56,267 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 2876.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:25:56,268 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 4173.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:25:57,606 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1332.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9847628474235535, 'Translated Text': 'Ce chien est bien cool, il ne peut pas assez de lui'}\n",
      "Sending tweet request... : Is a dog really the best pet?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:26:00,451 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4179.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:26:00,448 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 2839.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:26:00,450 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4178.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:26:01,796 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1341.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.998790442943573, 'Translated Text': \"Est-ce qu'un chien est vraiment le meilleur animal de compagnie?\"}\n",
      "Sending tweet request... : Cats are better than dogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:26:04,594 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4141.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m INFO 2023-02-15 17:26:04,593 translate_model translate_model#htTORx replica.py:505 - HANDLE __call__ OK 2794.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:26:04,594 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 4140.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:26:05,931 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1332.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9986716508865356, 'Translated Text': 'Les chats sont meilleurs que les chiens'}\n",
      "Sending tweet request... : Totally dissastified with the dog. Worst dog ever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:26:08,612 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4015.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71340)\u001b[0m INFO 2023-02-15 17:26:08,610 translate_model translate_model#htTORx replica.py:505 - HANDLE __call__ OK 2676.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:26:08,611 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4013.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:26:09,935 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1317.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.9998103976249695, 'Translated Text': 'Tr√®s d√©sassass√© avec le chien, pire chien jamais'}\n",
      "Sending tweet request... : Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:26:12,902 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4288.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:26:12,899 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 2962.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71361)\u001b[0m INFO 2023-02-15 17:26:12,902 ComposedModel ComposedModel#OHHnJD replica.py:505 - HANDLE __call__ OK 4287.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:sentiment_model pid=71308)\u001b[0m INFO 2023-02-15 17:26:14,244 sentiment_model sentiment_model#sFbEYV replica.py:505 - HANDLE __call__ OK 1337.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.9929038882255554, 'Translated Text': 'Le chien briliant lise mes humeurs comme un livre, ressent mes humeurs et r√©agit.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=71303)\u001b[0m INFO 2023-02-15 17:26:17,387 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4482.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:translate_model pid=71339)\u001b[0m INFO 2023-02-15 17:26:17,386 translate_model translate_model#MjgZhG replica.py:505 - HANDLE __call__ OK 3138.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=71362)\u001b[0m INFO 2023-02-15 17:26:17,386 ComposedModel ComposedModel#inGWxF replica.py:505 - HANDLE __call__ OK 4480.8ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    print(F\"Sending tweet request... : {tweet}\")\n",
    "    resp = requests.get(\"http://127.0.0.1:8000/composed\", params={'data': tweet})\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,031\tINFO deployment_state.py:1242 -- Removing 1 replicas from deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,034\tINFO deployment_state.py:1242 -- Removing 2 replicas from deployment 'translate_model'. component=serve deployment=translate_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=94973)\u001b[0m 2022-06-08 19:32:26,038\tINFO deployment_state.py:1242 -- Removing 2 replicas from deployment 'ComposedModel'. component=serve deployment=ComposedModel\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets to `TWEETS` with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476075b-d538-4ba4-84fa-02688b013a4d",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "We'll further explore model composition using [Deploymant Graph APIs](https://docs.ray.io/en/latest/serve/deployment-graph.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d2797-a9ca-4f2f-bcd2-477667109fac",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
